{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830\n",
      "  Cloning https://github.com/huggingface/transformers (to revision 21fac7abba2a37fae86106f87fcf9974fd1e3830) to c:\\users\\acer\\appdata\\local\\temp\\pip-req-build-e4621up0\n",
      "  Resolved https://github.com/huggingface/transformers to commit 21fac7abba2a37fae86106f87fcf9974fd1e3830\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: accelerate in d:\\anacoda\\lib\\site-packages (1.2.1)\n",
      "Collecting flash_attn\n",
      "  Using cached flash_attn-2.7.2.post1.tar.gz (3.1 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\acer\\AppData\\Local\\Temp\\pip-req-build-e4621up0'\n",
      "  Running command git rev-parse -q --verify 'sha^21fac7abba2a37fae86106f87fcf9974fd1e3830'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers 21fac7abba2a37fae86106f87fcf9974fd1e3830\n",
      "  Running command git checkout -q 21fac7abba2a37fae86106f87fcf9974fd1e3830\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [22 lines of output]\n",
      "      fatal: not a git repository (or any of the parent directories): .git\n",
      "      \n",
      "      \n",
      "      torch.__version__  = 2.5.1+cpu\n",
      "      \n",
      "      \n",
      "      C:\\Users\\acer\\AppData\\Local\\Temp\\pip-install-f28qzyr0\\flash-attn_41bd57df1a1f4b36bad49fc6aac24e1a\\setup.py:99: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n",
      "        warnings.warn(\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\acer\\AppData\\Local\\Temp\\pip-install-f28qzyr0\\flash-attn_41bd57df1a1f4b36bad49fc6aac24e1a\\setup.py\", line 177, in <module>\n",
      "          CUDAExtension(\n",
      "        File \"d:\\anacoda\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1078, in CUDAExtension\n",
      "          library_dirs += library_paths(cuda=True)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"d:\\anacoda\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1216, in library_paths\n",
      "          paths.append(_join_cuda_home(lib_dir))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"d:\\anacoda\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 2416, in _join_cuda_home\n",
      "          raise OSError('CUDA_HOME environment variable is not set. '\n",
      "      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830 accelerate flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1826490617.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install qwen-vl-utils[decord]\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# It's highly recommanded to use `[decord]` feature for faster video loading.\n",
    "pip install qwen-vl-utils[decord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "              \"type\": \"video\",\n",
    "              \"video\": \"/content/test.mp4\",\n",
    "              \"max_pixels\": 360*420,\n",
    "              \"fps\": 1.0,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Can you explain the video in brief?\"\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize = False, add_generation_prompt = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inputs, video_inputs = process_vision_info(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text = [text],\n",
    "    images = image_inputs,\n",
    "    videos = video_inputs,\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
